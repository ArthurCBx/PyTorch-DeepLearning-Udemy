{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKYB636n3kOS/QYrnNUJoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurCBx/PyTorch-DeepLearning-Udemy/blob/main/07_Experiment_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 07 PyTorch Experiment Tracking\n",
        "\n",
        "Machine learning is very experimental.\n",
        "\n",
        "In order to figure out which experiments are worth pursuing, that's where **experiment tracking** comes in, it helps you to figure out what doesn't work so you can figure out what **does work**\n",
        "\n",
        "In this notebook, we're going to see an example of programmatically tracking experiments.\n",
        "\n"
      ],
      "metadata": {
        "id": "xReiDhCz6lZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TiIZ4dR3SN1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "b3beqkUa8Ex3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "ufeL021P9Sfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed: int=42):\n",
        "  \"\"\"Sets random seeds for torch operations\n",
        "\n",
        "  Args:\n",
        "    Seed(int,optional): Random seed to set. Defaults to 42.\n",
        "  \"\"\"\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "J4HvON-39dgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()"
      ],
      "metadata": {
        "id": "gBwBo7-e9zVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get data\n",
        "\n",
        "Want to get pizza, steak sushi images.\n",
        "\n",
        "So we can run experiments building FoodVision Mini and see wich model performs best."
      ],
      "metadata": {
        "id": "PXBZoEVTA9vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "def download_data(source: str,\n",
        "                  destination: str,\n",
        "                  remove_source: bool = True) -> Path:\n",
        "  \"\"\"Downloads a zipped dataset from source and unzips to destination.\"\"\"\n",
        "  data_path = Path(\"data/\")\n",
        "  image_path = data_path / destination\n",
        "\n",
        "  if image_path.is_dir():\n",
        "    print(f\"[INFO] {image_path} directory already exists, skipping download.\")\n",
        "  else:\n",
        "    print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  target_file = Path(source).name\n",
        "  with open(data_path / target_file, \"wb\") as f:\n",
        "    request = requests.get(source)\n",
        "    print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "  with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
        "    print(f\"[INFO] Unzipping {target_file} data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "  if remove_source:\n",
        "    os.remove(data_path / target_file)\n",
        "\n",
        "  return image_path"
      ],
      "metadata": {
        "id": "YYOnz-DzBK_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "4iT3N6bNDEQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "7-pg7gZTEJEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Create DataLoaders with manual transforms\n",
        "\n",
        "The goal with transforms is to ensure your custom data is formatted in a reproducible way as well as a way that will suit pretrained models."
      ],
      "metadata": {
        "id": "f4jzm7cdEOT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directories\n",
        "train_dir = image_path  / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "vZq8qxH9EcOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ImageNet normalization levels\n",
        "from torchvision import transforms\n",
        "normalize = transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "                                 std=[0.229,0.224,0.225])\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=manual_transforms,\n",
        "                                                                               batch_size=32)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "u0hD2s7KFSww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Create DataLoader using automatically created transforms\n",
        "\n",
        "The same principle applies for automatic transforms: we want our custom data in the same format as a pretrained model was trained on."
      ],
      "metadata": {
        "id": "ZpvotT2BGhr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "auto_transform = weights.transforms()\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=auto_transform,\n",
        "                                                                               batch_size=32)"
      ],
      "metadata": {
        "id": "giVe78cmF5VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Getting a pretrained model, freeze the base layers and change the classifier head"
      ],
      "metadata": {
        "id": "Okw8Vty_dKWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: this is how a pretrained model would be created prior to torchvision v0.13\n",
        "#model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n",
        "\n",
        "# Download the pretrained weighs for EfiicientNet_B0\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "\n",
        "# Setup the model with the pretrained weights and send it to the target device\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "394hez5jdPLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all base layers by setting their requires_grad attribute to False\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "QkSfK8XseFYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=len(class_names)\n",
        "                    ).to(device)\n",
        ")"
      ],
      "metadata": {
        "id": "DIsd5Rqded3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchinfo"
      ],
      "metadata": {
        "id": "131ggssQffKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model=model,\n",
        "        input_size=(32,3,224,224),\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "kL7E1ZiYe9mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train a single model and track results"
      ],
      "metadata": {
        "id": "q3gGJmqwA2JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(),\n",
        "                             lr=0.001)"
      ],
      "metadata": {
        "id": "BT93sL4gA7s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To track experiments, we're going to use TensorBoard: https://www.tensorflow.org/tensorboard/\n",
        "\n",
        "And to interact with Tensorboard, we can use PyTorch's SummaryWritter - https://pytorch.org/docs/stable/tensorboard.html"
      ],
      "metadata": {
        "id": "3OswwAxUBlEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a SummaryWriter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "writer"
      ],
      "metadata": {
        "id": "SKrPcD9iBL8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from going_modular.going_modular.engine import train_step,test_step\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "              train_acc: [...],\n",
        "              test_loss: [...],\n",
        "              test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "             {train_loss: [2.0616, 1.0537],\n",
        "              train_acc: [0.3945, 0.3945],\n",
        "              test_loss: [1.2641, 1.5706],\n",
        "              test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Make sure model on target device\n",
        "    model.to(device)\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "        ### New: Experiment tracking ###\n",
        "        writer.add_scalars(main_tag=\"Loss\",\n",
        "                           tag_scalar_dict={\"train_loss\": train_loss,\n",
        "                                            \"test_loss\": test_loss},\n",
        "                           global_step=epoch)\n",
        "\n",
        "        writer.add_scalars(main_tag=\"Accuracy\",\n",
        "                           tag_scalar_dict={\"train_acc\": train_acc,\n",
        "                                            \"test_acc\": test_acc},\n",
        "                           global_step=epoch)\n",
        "\n",
        "        writer.add_graph(model=model,\n",
        "                         input_to_model=torch.randn(32,3,224,224).to(device))\n",
        "\n",
        "    # Close the writer\n",
        "    writer.close()\n",
        "    ### End new ###\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "e3CIrpGZCWOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "# Note: not using engine.train(), since we updated the train() function above\n",
        "set_seeds()\n",
        "results = train(model=model,\n",
        "                train_dataloader=train_dataloader,\n",
        "                test_dataloader=test_dataloader,\n",
        "                optimizer=optimizer,\n",
        "                loss_fn=loss_fn,\n",
        "                epochs=5,\n",
        "                device=device)"
      ],
      "metadata": {
        "id": "WPnm1njvFjim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. View our model's results with TensorBoard\n",
        "\n",
        "There are a few ways to view TensorBoard results, see them here: https://www.learnpytorch.io/07_pytorch_experiment_tracking/#5-view-our-models-results-in-tensorboard"
      ],
      "metadata": {
        "id": "2r2CsP7NJEM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's view our experiments from within the notebook\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "APOs42tKJBdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Create a function to prepare a `SummaryWriter()` instance\n",
        "\n",
        "By default our `SummaryWriter()` class saves to `log_dir`.\n",
        "\n",
        "How about if we wanted to save different experiments to different folders?\n",
        "\n",
        "In essence, one experiment = one folder.\n",
        "\n",
        "For example, we'd like to track:\n",
        "* Experiment date/timestamp\n",
        "* Experiment name\n",
        "* Model name\n",
        "* Extra - is there anything else that should be tracked?\n",
        "\n",
        "Let's create a function to cerate a `SummaryWriter()` instance to take all of these things into account.\n",
        "\n",
        "So ideally we end up tracking experiments to a directory:\n",
        "\n",
        "`runs/YYYY-MM-DD/experiment_name/model_name/extra`"
      ],
      "metadata": {
        "id": "NZ4WsaJHMH70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_writer(experiment_name: str,\n",
        "                  model_name: str,\n",
        "                  extra: str = None):\n",
        "  \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance tracking to a specific directory.\"\"\"\n",
        "\n",
        "  from datetime import datetime\n",
        "  import os\n",
        "\n",
        "  # Get timestamp of current date in reverse order\n",
        "  timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "  if extra:\n",
        "    # Create log directory path\n",
        "    log_dir = os.path.join(\"runs\",timestamp,experiment_name,model_name,extra)\n",
        "  else:\n",
        "    log_dir = os.path.join(\"runs\",timestamp,experiment_name,model_name)\n",
        "\n",
        "  print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
        "  return SummaryWriter(log_dir=log_dir)\n"
      ],
      "metadata": {
        "id": "RanvEf56L87e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_writer = create_writer(experiment_name=\"data_10_percent\",\n",
        "                               model_name=\"effnetb0\",\n",
        "                               extra=\"5_epochs\")\n",
        "\n",
        "example_writer"
      ],
      "metadata": {
        "id": "1oi-bOaiOqE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Update the `train()` function to include a `writer` parameter"
      ],
      "metadata": {
        "id": "vtHiQIVuTo5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update train() to use create_writer()\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from going_modular.going_modular.engine import train_step,test_step\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device,\n",
        "          writer: torch.utils.tensorboard.writer.SummaryWriter) -> Dict[str, List]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "              train_acc: [...],\n",
        "              test_loss: [...],\n",
        "              test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "             {train_loss: [2.0616, 1.0537],\n",
        "              train_acc: [0.3945, 0.3945],\n",
        "              test_loss: [1.2641, 1.5706],\n",
        "              test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Make sure model on target device\n",
        "    model.to(device)\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "        ### New: Experiment tracking ###\n",
        "        if writer:\n",
        "          writer.add_scalars(main_tag=\"Loss\",\n",
        "                             tag_scalar_dict={\"train_loss\": train_loss,\n",
        "                                              \"test_loss\": test_loss},\n",
        "                             global_step=epoch)\n",
        "\n",
        "          writer.add_scalars(main_tag=\"Accuracy\",\n",
        "                             tag_scalar_dict={\"train_acc\": train_acc,\n",
        "                                              \"test_acc\": test_acc},\n",
        "                             global_step=epoch)\n",
        "\n",
        "          writer.add_graph(model=model,\n",
        "                           input_to_model=torch.randn(32,3,224,224).to(device))\n",
        "\n",
        "          # Close the writer\n",
        "          writer.close()\n",
        "        else:\n",
        "          pass\n",
        "    ### End new ###\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "ePdpjrDOO8bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Setting up a series of modelling experiments\n",
        "\n",
        "* Setup 2x modelling experiments with effmetn0, pizza, steak, sushi data and train one model for 5 epochs and another model for 10 epochs"
      ],
      "metadata": {
        "id": "nXJOzZ68UgO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 What kind of experiments should you run?\n",
        "\n",
        "The number of machine learning experiments you can run, is like the number of different models you can build... almost limitless.\n",
        "\n",
        "However, you can't test everything...\n",
        "\n",
        "So, what should you test?\n",
        "* Change the number of epochs\n",
        "* Change the number of hidden units\n",
        "* Change the amount of data (right now we're using 10% of the Food101 dataset for pizza, steak sushi)\n",
        "* Change the learning rate\n",
        "* Try different kinds of data augmentation\n",
        "* Choose a different model architecture\n",
        "\n",
        "This is why transfer learning is so powerful, because, it's a working model that you can apply to your own problem."
      ],
      "metadata": {
        "id": "45w2H-4lM2qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 What experiments are we going to run?\n",
        "\n",
        "We're going to turn three dials:\n",
        "1. Model size - EffnetB0 vs EffnetB2\n",
        "2. Dataset size - 10% of pizza, steak, sushi images vs 20% (generally more data = better results)\n",
        "3. Training time - 5 epochs vs 10 epochs\n",
        "\n",
        "To begin, we're still keeping relatively small so that our experiments run quickly.\n",
        "\n",
        "**Our goal:** a model that os well performing but still small enough to run on a mobile device or browser, so FoodVision Mini can come to life.\n",
        "\n",
        "If you had infinite compute + time, you should basically always choose the biggest model and biggest dataset you can. http://www.incompleteideas.net/IncIdeas/BitterLesson.html"
      ],
      "metadata": {
        "id": "cJ8St2t6OFbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Download different datasets\n",
        "\n",
        "We want two datasets:\n",
        "1. Pizza, steak, sushi 10% - https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\n",
        "2. Pizza, steak, sushi 20% - https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\n",
        "\n"
      ],
      "metadata": {
        "id": "WVMgNZhNRiRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 10% and 20% datasets\n",
        "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                                     destination=\"pizza_steak_sushi\")\n",
        "\n",
        "data_20_percent_path = download_data(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")"
      ],
      "metadata": {
        "id": "0qYNl0T5T0sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Transform Datasets and Create DataLoaders\n",
        "\n",
        "we'll need to transform our data so that they have the same format as the data our pretrained model was trained on."
      ],
      "metadata": {
        "id": "VMGB2y4yTuBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_10_percent = data_10_percent_path / \"train\"\n",
        "train_dir_20_percent = data_20_percent_path / \"train\"\n",
        "\n",
        "# Setup the test directory used for both models\n",
        "test_dir = data_10_percent_path / \"test\"\n",
        "\n",
        "train_dir_10_percent,train_dir_20_percent, test_dir"
      ],
      "metadata": {
        "id": "54sRtDCXTfTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n"
      ],
      "metadata": {
        "id": "bFKN-WURUbM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "\n",
        "# Create 10% training and test DataLoaders\n",
        "train_dataloader_10_percent,test_dataloader,class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
        "                                                                                        test_dir=test_dir,\n",
        "                                                                                        transform=simple_transform,\n",
        "                                                                                        batch_size=BATCH_SIZE)\n",
        "\n",
        "# Create 20% training and test DataLoaders\n",
        "train_dataloader_20_percent,test_dataloader,class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
        "                                                                                        test_dir=test_dir,\n",
        "                                                                                        transform=simple_transform,\n",
        "                                                                                        batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 10% train data: {len(train_dataloader_10_percent)}\")\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 20% train data: {len(train_dataloader_20_percent)}\")\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 10% test data: {len(test_dataloader)}\")\n",
        "print(f\"Class names: {class_names}\")"
      ],
      "metadata": {
        "id": "XPQIZOkuUvbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5 Create feature extractor models\n",
        "\n",
        "We want two functions:\n",
        "1. Creates a `torchvision.models.efficientnet_b0()` feature extractor with a frozen backbone/base layers and a custom classifier head.\n",
        "2. Creates a `torchvision.models.efficientnet_b2()` feature extractor with a frozen backbone/base layers and a custom classifier head."
      ],
      "metadata": {
        "id": "5rfUgiJNWck6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)"
      ],
      "metadata": {
        "id": "S2qArfl4WYWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "OUT_FEATURES = len(class_names)\n",
        "\n",
        "def create_effnetb0():\n",
        "  # Get the weights and setup a model\n",
        "  weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "  model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "\n",
        "  # Freeze the base model layers\n",
        "  for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Change the classifier head\n",
        "  set_seeds()\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.2, inplace=True),\n",
        "      nn.Linear(in_features=1280,out_features=OUT_FEATURES)\n",
        "  ).to(device)\n",
        "\n",
        "  # Give the model a name\n",
        "  model.name = \"effnetb0\"\n",
        "  print(f\"[INFO] Created new {model.name} model...\")\n",
        "  return model\n",
        "\n",
        "def create_effnetb2():\n",
        "  # Get the weights and setup a model\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
        "\n",
        "  # Freeze the base model layers\n",
        "  for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Change the classifier head\n",
        "  set_seeds()\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408,out_features=OUT_FEATURES)\n",
        "  ).to(device)\n",
        "\n",
        "  # Give the model a name\n",
        "  model.name = \"effnetb2\"\n",
        "  print(f\"[INFO] Created new {model.name} model...\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "86vDh8haY3bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "created_model_test_effnetb0 = create_effnetb0()\n",
        "created_model_test_effnetb2 = create_effnetb2()\n",
        "\n",
        "summary(model=created_model_test_effnetb0,\n",
        "        input_size=(32,3,224,224),\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eGUHD-XEbBN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=created_model_test_effnetb2,\n",
        "        input_size=(32,3,224,224),\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "USjsDOh7bQcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.6 Create experiments and set up training code"
      ],
      "metadata": {
        "id": "fbqoL_kGhXKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create epoch list\n",
        "num_epochs = [5,10]\n",
        "\n",
        "# Create models list\n",
        "models = [\"effnetb0\", \"effnetb2\"]\n",
        "\n",
        "# Create a DataLoaders dictionary\n",
        "train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n",
        "                     \"data_20_percent\": train_dataloader_20_percent}\n"
      ],
      "metadata": {
        "id": "z_yTJkFQhWey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from going_modular.going_modular.utils import save_model\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "# Keep track of experiment numbers\n",
        "experiment_number = 0\n",
        "\n",
        "# Loop through each DataLoader\n",
        "for dataloader_name, train_dataloader in train_dataloaders.items():\n",
        "  #Loop through the epochs\n",
        "  for epoch in num_epochs:\n",
        "    # Loop through each model name and create a new model instance\n",
        "    for model_name in models:\n",
        "\n",
        "      #Print out info\n",
        "      experiment_number += 1\n",
        "      print(f\"[INFO] Experiment number: {experiment_number}\")\n",
        "      print(f\"[INFO] Model : {model_name}\")\n",
        "      print(f\"[INFO] DataLoader : {dataloader_name}\")\n",
        "      print(f\"[INFO] Number of epochs: {epoch}\")\n",
        "\n",
        "      # Select and create the model\n",
        "      if model_name == \"efnetb0\":\n",
        "        model = create_effnetb0()\n",
        "      else:\n",
        "        model = create_effnetb2()\n",
        "\n",
        "      # Create a new loss and optimizer for every model\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.Adam(params=model.parameters(),lr=0.001)\n",
        "\n",
        "      # Train target model with target dataloader\n",
        "      train(model=model,\n",
        "            train_dataloader=train_dataloader,\n",
        "            test_dataloader=test_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            loss_fn=loss_fn,\n",
        "            epochs=epoch,\n",
        "            device=device,\n",
        "            writer=create_writer(experiment_name=dataloader_name,\n",
        "                                 model_name=model_name,\n",
        "                                 extra=f\"{epoch}_epochs\"))\n",
        "\n",
        "      # Save the model to file so we can import it later if need be\n",
        "      save_filepath = f\"07_{model_name}_{dataloader_name}_{epoch}_epochs.pth\"\n",
        "      save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=save_filepath)\n",
        "      print(\"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "rFFfBrg7h-mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. View experiments in TensorBoard\n",
        "\n",
        "We've experimented, now let's visualize"
      ],
      "metadata": {
        "id": "bYm2PXWKUsIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "3t13It_Fmc-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the results to TensorBoard.dev\n",
        "!tensorboard dev upload --logdir runs \\\n",
        "    --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n",
        "    --description \"Comparing results of different model size, training data amount and training time.\""
      ],
      "metadata": {
        "id": "w1XVlxHeWMaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Load in the best performing model and make predictions with it\n",
        "\n",
        "This is our best model filepath: `models/07_effnetb2_data_20_percent_10_epochs.pth`"
      ],
      "metadata": {
        "id": "qnqOuv6sYcxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n",
        "\n",
        "best_model = create_effnetb2()\n",
        "best_model.load_state_dict(torch.load(best_model_path))"
      ],
      "metadata": {
        "id": "ADqHnhW-X9bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal: create a FoodVision Mini model that performs well enough and is able to run on a mobile device/web browser"
      ],
      "metadata": {
        "id": "VXyZTnftZXbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model file size\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert it to megabytes\n",
        "effnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\n",
        "print(f\"[INFO] EffNetB2 model size: {effnetb2_model_size} MB\")"
      ],
      "metadata": {
        "id": "zX512JfcZRs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import function to make predictions on images\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "# Get a random list of 3 image path names from the test dataset\n",
        "import random\n",
        "num_images_to_plot = 3\n",
        "test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\"))\n",
        "test_image_path_sample = random.sample(test_image_path_list,k=num_images_to_plot)\n",
        "\n",
        "# Make and plot predictions on the test dataset\n",
        "for image_path in test_image_path_sample:\n",
        "  pred_and_plot_image(model=best_model,\n",
        "                      class_names=class_names,\n",
        "                      image_path=image_path,\n",
        "                      image_size=(224,224))"
      ],
      "metadata": {
        "id": "YujCVPIRaJyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Predict on a custom image"
      ],
      "metadata": {
        "id": "4Ksn6Dt8bh6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_pizza_path = Path(\"data/custom_image/pizza_custom.png\")\n",
        "pred_and_plot_image(model=best_model,\n",
        "                    class_names=class_names,\n",
        "                    image_path=\"data/custom_image/pizza_custom.png\",\n",
        "                    )"
      ],
      "metadata": {
        "id": "BR4qfk3TbgGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "91e0kz3jlJLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.EfficientNet_B7_Weights.DEFAULT\n",
        "effnetb7_5_epochs_20_percent = torchvision.models.efficientnet_b7(weights=weights).to(device)\n",
        "effnetb7_10_epochs_20_percent =torchvision.models.efficientnet_b7(weights=weights).to(device)\n",
        "models = [effnetb7_5_epochs_20_percent,effnetb7_10_epochs_20_percent]\n",
        "effnetb7_10_epochs_20_percent\n",
        "\n",
        "for model in models:\n",
        "  for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.5, inplace=True),\n",
        "      nn.Linear(in_features=2560,out_features=len(class_names))\n",
        "  )\n",
        "loss_fn_5_epochs = nn.CrossEntropyLoss()\n",
        "loss_fn_10_epochs = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_5_epochs = torch.optim.Adam(params=effnetb7_5_epochs_20_percent.parameters(),lr=0.001)\n",
        "optimizer_10_epochs = torch.optim.Adam(params=effnetb7_10_epochs_20_percent.parameters(),lr=0.001)\n",
        "\n",
        "\n",
        "train(model=effnetb7_5_epochs_20_percent,\n",
        "      train_dataloader=train_dataloader_20_percent,\n",
        "      test_dataloader=test_dataloader,\n",
        "      optimizer=optimizer_5_epochs,\n",
        "      loss_fn=loss_fn_5_epochs,\n",
        "      epochs=5,\n",
        "      device=device,\n",
        "      writer=create_writer(experiment_name=\"5_epochs\",\n",
        "                           model_name=\"effnetb7_5_epochs_20_percent\"))\n",
        "\n",
        "train(model=effnetb7_5_epochs_20_percent,\n",
        "      train_dataloader=train_dataloader_20_percent,\n",
        "      test_dataloader=test_dataloader,\n",
        "      optimizer=optimizer_10_epochs,\n",
        "      loss_fn=loss_fn_10_epochs,\n",
        "      epochs=10,\n",
        "      device=device,\n",
        "      writer=create_writer(experiment_name=\"10_epochs\",\n",
        "model_name=\"effnetb7_10_epochs_20_percent\"))\n"
      ],
      "metadata": {
        "id": "aCqleOcrUQ3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    train_transform: transforms.Compose,\n",
        "    test_transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training directory and testing directory path and turns\n",
        "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = \\\n",
        "        = create_dataloaders(train_dir=path/to/train_dir,\n",
        "                             test_dir=path/to/test_dir,\n",
        "                             transform=some_transform,\n",
        "                             batch_size=32,\n",
        "                             num_workers=4)\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n",
        "\n"
      ],
      "metadata": {
        "id": "xX-xGk0AaCqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "augmented_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.TrivialAugmentWide(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "train_dataloader_20_percent_aug, test_dataloader, class_names = create_dataloaders(train_dir=train_dir_20_percent,\n",
        "                                                                                   test_dir=test_dir,\n",
        "                                                                                   train_transform=augmented_transform,\n",
        "                                                                                   test_transform=simple_transform,\n",
        "                                                                                   batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "jV9Np3jUbFBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "effnetb2_data_20_percent_10_epochs_augmented = create_effnetb2()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=effnetb2_data_20_percent_10_epochs_augmented.parameters(),lr=0.001)\n",
        "\n",
        "train(model=effnetb2_data_20_percent_10_epochs_augmented,\n",
        "      train_dataloader=train_dataloader_20_percent_aug,\n",
        "      test_dataloader=test_dataloader,\n",
        "      optimizer=optimizer,\n",
        "      loss_fn=loss_fn,\n",
        "      epochs=10,\n",
        "      device=device,\n",
        "      writer=create_writer(experiment_name=\"data_20_percent_augmented\",\n",
        "                           model_name=\"effnetb2\",extra=\"10_epochs\"))"
      ],
      "metadata": {
        "id": "0x8kBLAKbmvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "qIYVw73Tc2fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_food101 = Path(\"data/food101_correct\")\n",
        "train_data = datasets.Food101(root=correct_food101,\n",
        "                              split=\"train\",\n",
        "                              # transform=transforms.ToTensor(),\n",
        "                              download=True)\n",
        "\n",
        "# Get testing data\n",
        "test_data = datasets.Food101(root=correct_food101,\n",
        "                             split=\"test\",\n",
        "                             # transform=transforms.ToTensor(),\n",
        "                             download=True)"
      ],
      "metadata": {
        "id": "q5fZXxrFl45e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLedDCNvs9kM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}