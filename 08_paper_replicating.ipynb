{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurCBx/PyTorch-DeepLearning-Udemy/blob/main/08_paper_replicating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VikhhrcOeVt5"
      },
      "source": [
        "# 08. Milestone Project 2: PyTorch Paper Replicating\n",
        "\n",
        "The goal of machine learning research paper replicating is: turn a ML research paper into usable code.\n",
        "\n",
        "In this notebook, we're going to be replicating the Vision Transformer (ViT) architecture/paper with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReE63p11e4yf"
      },
      "source": [
        "## 0. Get setup\n",
        "\n",
        "Let's import code we've previously written + required library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbzXBRe14TfF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVCHPVYSfmXF"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7TWxHZBfuVg"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "  !mv pytorch-deep-learning/helper_functions.py .\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "  from helper_functions import download_data, set_seeds, plot_loss_curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL6kd99QgMdE"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvvC-xklgqNg"
      },
      "source": [
        "## 1. Get data\n",
        "\n",
        "The whole goal of what we're trying to do is to replicate the ViT architecture for our FoodVision Mini problem.\n",
        "\n",
        "To do that, we need some data.\n",
        "\n",
        "Namely, the pizza, steak and sushi images we've been using so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pAdfDK3hIII"
      },
      "outputs": [],
      "source": [
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9L0HgvdiIfL"
      },
      "outputs": [],
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LMKOie0z2uY"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxPadWxHz1UJ"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "from torchvision import transforms\n",
        "# Create image size\n",
        "IMG_SIZE = 224 # Comes from Table 3 of the ViT paper\n",
        "\n",
        "# Create transforms pipeline\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBNxvObA1SFu"
      },
      "outputs": [],
      "source": [
        "# Create a batch size of 32 (the paper uses 4096 but this may be too big for our smaller problem)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir,\n",
        "                                                                               test_dir,\n",
        "                                                                               manual_transforms,\n",
        "                                                                               BATCH_SIZE)\n",
        "\n",
        "len(train_dataloader), len(test_dataloader), class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4d_72go142_"
      },
      "source": [
        "### 2.3 Visualize a single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6p6a7Zh17NN"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "print(f\"Image batch shape: {image_batch.shape}\")\n",
        "print(f\"Label batch shape: {label_batch.shape}\")\n",
        "\n",
        "# Get a single image and label from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "plt.imshow(image.permute(1,2,0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXns5evw5sLM"
      },
      "source": [
        "## 3. Replicating ViT: Overview\n",
        "\n",
        "Looking at a whole machine learning research paper can be intimidating.\n",
        "\n",
        "So, in order to make it more understandable, we can break it down into smaller pieces:\n",
        "\n",
        "* **Inputs** - What goes into the model? (in our case, image tensors)\n",
        "* **Outputs** - What comes out of the model/layer/block? (in our case, we want the model to ouput image classification labels)\n",
        "* **Layers** - Takes an input, manipulates it with a function (for example could be self-attention).\n",
        "* **Blocks** - A collection of layers.\n",
        "* **Model (or achitecture)** - A collection of blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF-o6b457lw7"
      },
      "source": [
        "### 3.1 ViT overview: pieces of the puzzle\n",
        "\n",
        "* Figure 1: Visual overview of the architecture\n",
        "* Four equations: math equations which define the functions of each layer/block\n",
        "* Table 1/3: different hyperparameters for the architecture/training.\n",
        "* Text\n",
        "\n",
        "### Figure 1\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-figure-1-architecture-overview.png)\n",
        "\n",
        "* Enbedding - learnable representation (start with random numbers and improve over time)\n",
        "\n",
        "### Four equations\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-four-equations.png)\n",
        "\n",
        "#### Section 3.1:\n",
        "\n",
        "**Equation 1:**\n",
        "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\n",
        "\n",
        "**Equation 1:**\n",
        "Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\n",
        "\n",
        "In pseudocode:\n",
        "\n",
        "```python\n",
        "x_input = [class_token, image_patch_1,...,image_patch_N] + [class_token_pos, image_patch_1_pos,...]\n",
        "```\n",
        "\n",
        "**Equations 2&3:**\n",
        "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski \\& Auli, 2019).\n",
        "\n",
        "In pseudocode:\n",
        "\n",
        "```python\n",
        "# Equation 2\n",
        "x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n",
        "\n",
        "# Equation 3\n",
        "x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n",
        "```\n",
        "\n",
        "**Equation 4:**\n",
        "Similar to BERT's [class] token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_0^0=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_L^0\\right)$ serves as the image representation $y$ (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to $\\mathbf{z}_L^0$. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.\n",
        "\n",
        "* MLP = multilayer perceptron = a neural network with X number of layers\n",
        "* MLP = one hidden layer at training time\n",
        "* MLP = single linear layer at fine-tuning time\n",
        "\n",
        "In pseudocode:\n",
        "```python\n",
        "# Equation 4\n",
        "y = MLP(LN_layer(x_output_MLP_block))\n",
        "```\n",
        "\n",
        "### Table 1\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-table-1.png)\n",
        "\n",
        "* ViT-Base, ViT-Large and ViT-Huge are all different sizes of the same model architecture.\n",
        "* Layers - the number of transformers encoder layers.\n",
        "* Hidden size $D$ - the embedding size throughout the architecture (size of the vector containing the patch).\n",
        "* MLP size - the number of hidden units/neurons in the MLP.\n",
        "* Head - the number of multi-head self-attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV4R5y0MCrGK"
      },
      "source": [
        "## 4. Equation 1: Split data into patches and creating the class, position and patch embedding\n",
        "\n",
        "Layers = input -> function -> output\n",
        "\n",
        "What's the input shape?\n",
        "\n",
        "What's the output shape?\n",
        "\n",
        "* Input shape: (224,224,3) -> single image -> (height, width, color channels\n",
        "* Output shape: ?\n",
        "\n",
        "### 4.1 Calculate input and output shape by hand\n",
        "\n",
        "* Input shape: $H\\times{W}\\times{C}$\n",
        "* Outupt shape: $\\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$\n",
        "* H = height\n",
        "* W = width\n",
        "* C = color channels\n",
        "* P = patch size\n",
        "* N = number of patches = (height * width) / p²\n",
        "* D = constant latent vector size = embedding dimension (see table 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwHKPDm47ck6"
      },
      "outputs": [],
      "source": [
        "# Create example values\n",
        "height = 224\n",
        "width = 224\n",
        "color_channels = 3\n",
        "patch_size = 16\n",
        "\n",
        "# Calculate the number of patches\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "number_of_patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zugA8Na6J4Vf"
      },
      "outputs": [],
      "source": [
        "# Input shape\n",
        "embedding_layer_input_shape = (height, width, color_channels)\n",
        "\n",
        "# Output_shape\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
        "\n",
        "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
        "print(f\"Output shape (single 1D sequence of patches): {embedding_layer_output_shape} -> (number_of_patches, embedding_dimension)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jymIUGOwK3bG"
      },
      "source": [
        "### 4.2 Turning a single image into patches\n",
        "\n",
        "Let's visualize,visualize,visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jW8O3EsK-Ke"
      },
      "outputs": [],
      "source": [
        "# View a single image\n",
        "plt.imshow(image.permute(1,2,0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1NfzZP5LkL9"
      },
      "outputs": [],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l05CU5w5LxAh"
      },
      "outputs": [],
      "source": [
        "# Get the top row of the image\n",
        "image_permuted = image.permute(1,2,0) # Convert image to color channles last (H,W,C)\n",
        "\n",
        "# Index to plot the top row of pixels\n",
        "patch_size = 16\n",
        "plt.figure(figsize=(patch_size,patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzAIp2FEMuVW"
      },
      "outputs": [],
      "source": [
        "# Setup code to plot top row as patches\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size}\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axis = plt.subplots(nrows=1,\n",
        "                        ncols=img_size // patch_size, # One column for each patch\n",
        "                        sharex=True,\n",
        "                        sharey=True,\n",
        "                        figsize=(patch_size,patch_size))\n",
        "\n",
        "# Iterate through number of patches in the top row\n",
        "\n",
        "for i, patch in enumerate(range(0,img_size, patch_size)):\n",
        "  axis[i].imshow(image_permuted[:patch_size,patch:patch+patch_size,:])\n",
        "  axis[i].set_xlabel(i)\n",
        "  axis[i].set_xticks([])\n",
        "  axis[i].set_yticks([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k82iHck7N-3Q"
      },
      "outputs": [],
      "source": [
        "# Setup code to plot whole image as patches\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\\n",
        "\\nNumber of patches per column: {num_patches}\\\n",
        "\\nTotal patches: {num_patches*num_patches}\\\n",
        "\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axis = plt.subplots(nrows=img_size // patch_size,\n",
        "                        ncols=img_size // patch_size, # One column for each patch\n",
        "                        sharex=True,\n",
        "                        sharey=True,\n",
        "                        figsize=(num_patches,num_patches))\n",
        "\n",
        "# Loop through height and width of image\n",
        "for i, patch_height in enumerate(range(0,img_size, patch_size)):\n",
        "  for j, patch_width in enumerate(range(0,img_size, patch_size)):\n",
        "    # Plot the permuted image on the different axes\n",
        "    axis[i,j].imshow(image_permuted[patch_height:patch_height+patch_size,\n",
        "                                    patch_width:patch_width+patch_size, # iterate through width\n",
        "                                    :]) # Get all color channels\n",
        "    # Set up label information for each subplot (patch)\n",
        "    axis[i, j].set_ylabel(i+1,\n",
        "                          rotation=\"horizontal\",\n",
        "                          horizontalalignment=\"right\",\n",
        "                          verticalalignment=\"center\")\n",
        "\n",
        "    axis[i, j].set_xlabel(j+1)\n",
        "    axis[i, j].set_xticks([])\n",
        "    axis[i, j].set_yticks([])\n",
        "    axis[i,j].label_outer()\n",
        "\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchfied\",fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaA3maLxjR8"
      },
      "source": [
        "### 4.3 Creating image patches and turning them into patch embeddings\n",
        "\n",
        "Perhaps we could create the image patches and image patch embeddings in a single step using `torch.nn.Conv2d()` and setting the kernel size and stride parameters to `patch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC-N1NxgzOmg"
      },
      "outputs": [],
      "source": [
        "# Create conv2d layer to turn image into patches of learnable feature maps (embeddings)\n",
        "from torch import nn\n",
        "\n",
        "# Set the patch size\n",
        "patch_size=16\n",
        "\n",
        "# Create a conv2d layer with hyperparameters from the ViT paper\n",
        "conv2d = nn.Conv2d(in_channels=3,\n",
        "                   out_channels=768, # D size from table 1 (patch_size² * 3)\n",
        "                   kernel_size=patch_size,\n",
        "                   stride=patch_size,\n",
        "                   padding=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szBkGMT70abD"
      },
      "outputs": [],
      "source": [
        "# View a single image\n",
        "plt.imshow(image.permute(1,2,0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77qwzmf40jIx"
      },
      "outputs": [],
      "source": [
        "# Pass the image through the convolutional layer\n",
        "image_out_of_conv = conv2d(image.unsqueeze(dim=0)) # Add batch dimension\n",
        "print(f\"Shape of the output of the convolutional layer: {image_out_of_conv.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBnHB53L00Xy"
      },
      "source": [
        "14*14 is the total number of patches (196)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCDkEkV_1c1T"
      },
      "source": [
        "Now we've passed a single image to our `conv2d` layer, it's shape is:\n",
        "\n",
        "```python\n",
        "torch.Size([1,768,14,14]) # (batch_size, embedding_dim, feature_map_height, feature_map_width)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nblq7-EA063f"
      },
      "outputs": [],
      "source": [
        "# Plot random convolutional feature maps (embeddings)\n",
        "import random\n",
        "random_indexes = random.sample(range(0,758), k =5)\n",
        "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
        "\n",
        "# Create a plot\n",
        "fig, axis = plt.subplots(nrows=1,ncols=5,figsize=(12,12))\n",
        "\n",
        "# Plot random image feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "  image_conv_feature_map = image_out_of_conv[:,idx,:,:] # index on the output tensor of the conv2d layer\n",
        "  axis[i].imshow(image_conv_feature_map.squeeze().detach().numpy()) # remove batch dimension and remove from grad tracking/switch to numpy for matplotlib\n",
        "  axis[i].set(xticklabels=[],yticklabels=[],xticks=[],yticks=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl2BMyFh3r8-"
      },
      "outputs": [],
      "source": [
        "# Get a single feature map in tensor form\n",
        "single_feature_map = image_out_of_conv[:,0,:,:]\n",
        "single_feature_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Ot3Ape4LVz"
      },
      "source": [
        "### 4.4 Flattening the patch embedding with `torch.nn.Flaten()`\n",
        "\n",
        "Right now we've got a series of convolutional feature maps (patch embeddings) that we want to flatten into a sequence of patch embeddings to satisfy the input criteria of the ViT Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA3-japp4Ksr"
      },
      "outputs": [],
      "source": [
        "print(f\"{image_out_of_conv.shape} -> (batch_size, embedding_dim, feature_map_height, feature_map_width)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7U5ss7j-L1q"
      },
      "source": [
        "Want: (batch_size, number_of_patches, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciR5o6Sz-Jp5"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "flatten_layer = nn.Flatten(start_dim=2,\n",
        "                            end_dim=3)\n",
        "flatten_layer(image_out_of_conv).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB0gOVzn-pr5"
      },
      "outputs": [],
      "source": [
        "# Put everything together\n",
        "plt.imshow(image.permute(1,2,0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Original image shape: {image.shape}\")\n",
        "\n",
        "#Turn image into feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(dim=0)) # add batch dimension\n",
        "print(f\"Image feature map (patches) shape: {image_out_of_conv.shape}\")\n",
        "\n",
        "# Flatten the feature maps\n",
        "image_out_of_conv_flattened = flatten_layer(image_out_of_conv)\n",
        "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REo7eWtn_dWM"
      },
      "outputs": [],
      "source": [
        "# Rearrange output of flattened layer\n",
        "image_out_of_conv_flattened_permuted = image_out_of_conv_flattened.permute(0,2,1)\n",
        "print(f\"{image_out_of_conv_flattened_permuted.shape} -> (batch_size, number_of_patches, embedding_dim)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCf9yZZR_7QZ"
      },
      "outputs": [],
      "source": [
        "# Get a single flattened feature map\n",
        "single_flattened_feature_map = image_out_of_conv_flattened_permuted[: ,: ,0]\n",
        "\n",
        "# Plot the flattened feature map visually\n",
        "plt.figure(figsize=(22,22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
        "plt.axis(False)\n",
        "plt.title(f\"Single flattened feature map shape: {single_flattened_feature_map.shape}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrsjzSlEW7v7"
      },
      "source": [
        "### 4.5 Turning the ViT patch embedding layer into a PyTorch module\n",
        "\n",
        "We want this module to do a few things:\n",
        "1. Create a class called `PatchEmbedding` that inherits from `nn.Module`.\n",
        "2. Initialize with appropriate hyperparameters, such as channels, embedding dimension, patch size.\n",
        "3. Create a layer to turn an image into embedded patches using `nn.Conv2d()`.\n",
        "4. Create a layer to flatten the feature maps of the output of the layer in 3.\n",
        "5. Define a `forward()` that defines the forward computation (e.g. pass through layer from 3 and 4).\n",
        "6. Make sure the output shape of the layer reflects the required output shape of the patch embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuqv8021YCQT"
      },
      "outputs": [],
      "source": [
        "# 1. Create a class called PatchEmbedding\n",
        "class PatchEmbedding(nn.Module):\n",
        "  # 2. Initialize the layer with appropriate hyperparameters\n",
        "  def __init__(self,\n",
        "               input_size: int=3,\n",
        "               patch_size: int=16,\n",
        "               embedding_dim: int=768):\n",
        "    super().__init__()\n",
        "\n",
        "    # 3. Create a layer to turn an image\n",
        "    self.patcher = nn.Conv2d(in_channels=input_size,\n",
        "                             out_channels=embedding_dim,\n",
        "                             kernel_size=patch_size,\n",
        "                             stride=patch_size,\n",
        "                             padding=0)\n",
        "\n",
        "    # 4. Create a layer to flatten feature map outputs of Conv2d\n",
        "    self.flatten = nn.Flatten(start_dim=2,\n",
        "                              end_dim=3)\n",
        "\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  # 5. Define a forward method to define the forward computation steps\n",
        "  def forward(self,x):\n",
        "    # Create assertion to check that inputs are the correct shape\n",
        "    image_resolution = x.shape[-1]\n",
        "    assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
        "\n",
        "    # Perform the forward pass\n",
        "    x =  self.flatten(self.patcher(x))\n",
        "\n",
        "    # 6. Make sure the returned sequence embedding dimensions are in the right order (batch_size, number_of_patches, embedding dimension)\n",
        "    return x.permute(0,2,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEIUn4y1azA2"
      },
      "outputs": [],
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create an instance of patch embedding layer\n",
        "patchify = PatchEmbedding()\n",
        "\n",
        "# Pass a single image through patch embedding layer\n",
        "print(f\"Input image size: {image.unsqueeze(0).shape}\")\n",
        "patch_embedded_image = patchify(image.unsqueeze(0))\n",
        "print(f\"Output patch embedded sequence shape: {patch_embedded_image.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmhRivag3_V"
      },
      "source": [
        "### 4.6 Creating the class token embedding\n",
        "\n",
        "Want to: prepend a learnable class token to the start of the patch embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5gUA5VbgcfW"
      },
      "outputs": [],
      "source": [
        "patch_embedded_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYW1Et7ehk3Y"
      },
      "outputs": [],
      "source": [
        "# Get the batch size and embedding dimension\n",
        "batch_size = patch_embedded_image.shape[0]\n",
        "embedding_dim = patch_embedded_image.shape[-1]\n",
        "print(f\"Batch size: {batch_size}, embedding dimension: {embedding_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qa_vQWch120"
      },
      "outputs": [],
      "source": [
        "# Create a class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
        "class_token = nn.Parameter(torch.ones(batch_size,1,embedding_dim,requires_grad=True))\n",
        "class_token.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RayFtwY6iXdJ"
      },
      "outputs": [],
      "source": [
        "# Add the class token embedding to the front of the patch embedding\n",
        "patch_embedded_image_with_class_embedding = torch.cat((class_token,patch_embedded_image),\n",
        "                                                      dim=1) # Number_of_patches dimension\n",
        "\n",
        "print(patch_embedded_image_with_class_embedding)\n",
        "print(f\"Sequence of patch embeddings with class token prepended shape{patch_embedded_image_with_class_embedding.shape} -> (batch_size, class token + number_of_patches, embedding_dim)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeyAUujQjjDS"
      },
      "source": [
        "The first line represents the class of the image. Usually it'll start with random values to be optimized, but in our example it starts with ones just to make it more visuable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T57G6opu1h8d"
      },
      "source": [
        "### 4.7 Creating the position embedding\n",
        "\n",
        "Want to: create a series of 1D learnable position embeddings and to add them to the sequence of patch embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGmbdvBP1xlQ"
      },
      "outputs": [],
      "source": [
        "# View the sequence of patch embeddings with the prepended class embedding\n",
        "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Por2ztuY2MXM"
      },
      "outputs": [],
      "source": [
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height*width) / patch_size**2)\n",
        "\n",
        "# Get the embedding dimension\n",
        "embedding_dimension = patch_embedded_image_with_class_embedding.shape[-1]\n",
        "\n",
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.ones(1,\n",
        "                                             number_of_patches+1,\n",
        "                                             embedding_dimension),\n",
        "                                  requires_grad=True)\n",
        "position_embedding,position_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afkgVtY73A84"
      },
      "outputs": [],
      "source": [
        "# Add the position embedding to the patch and class token embedding\n",
        "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
        "patch_and_position_embedding, patch_and_position_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKZwqTB4vmi"
      },
      "source": [
        "### 4.8 Putting it all together: from image to embedding\n",
        "\n",
        "We've written code to turn an image in a flattened sequence of patch embeddings.\n",
        "\n",
        "Now let's see it all in one cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrahEfnQ5Hvg"
      },
      "outputs": [],
      "source": [
        "set_seeds()\n",
        "\n",
        "# 1. Set the patch size\n",
        "patch_size = 16\n",
        "\n",
        "# 2. Print shapes of the original image tensor and get the image dimensions\n",
        "print(f\"Image tensor shape: {image.shape}\")\n",
        "width, height = image.shape[-1], image.shape[2]\n",
        "\n",
        "# 3. Get the image tensor and add the batch dimension\n",
        "x = image.unsqueeze(0)\n",
        "print(f\"Input image shape: {x.shape}\")\n",
        "\n",
        "# 4. Create patch embedding layer\n",
        "patch_embedding_layer = PatchEmbedding(input_size=3,\n",
        "                                       patch_size=patch_size,\n",
        "                                       embedding_dim=768)\n",
        "\n",
        "# 5. Pass input image through PatchEmbedding\n",
        "patch_embedding = patch_embedding_layer(x)\n",
        "print(f\"Patch embedding shape: {patch_embedding.shape}\")\n",
        "\n",
        "# 6. Get the class token embedding\n",
        "batch_size = patch_embedding.shape[0]\n",
        "embedding_dimension = patch_embedding.shape[-1]\n",
        "class_token = nn.Parameter(torch.ones(batch_size,1,embedding_dimension),\n",
        "                           requires_grad=True)\n",
        "print(f\"Class token embedding shape: {class_token.shape}\")\n",
        "\n",
        "# 7. Prepend the class token embedding to the patch embedding\n",
        "patch_embedding_class_token = torch.cat((class_token, patch_embedding),dim=1)\n",
        "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
        "\n",
        "# 8. Create position embedding\n",
        "number_of_patches = int((height*width) / patch_size**2)\n",
        "position_embedding = nn.Parameter(torch.ones(1,\n",
        "                                             number_of_patches+1,\n",
        "                                             embedding_dimension),\n",
        "                                  requires_grad=True)\n",
        "\n",
        "# 9. Add the position embedding to patch embedding with class token\n",
        "patch_embedding_class_token_position = patch_embedding_class_token + position_embedding\n",
        "print(f\"Patch embedding with class token and position embedding shape: {patch_embedding_class_token_position.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIXkEZXLRZte"
      },
      "source": [
        "## Equation 2: Multihead Self-Attention (MSA Block)\n",
        "\n",
        "* **Multihead self-attention**: wich part of a sequence should pay the most attention to itself?\n",
        " * In our case, we have a series of embedded image patches, wich patch significantly relates to another patch.\n",
        " * We want our neural network (ViT) to learn this relationship/representation.\n",
        "* To replicate MSA in PyTorch we can use: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
        "* **LayerNorm** = Layer normalization is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy.\n",
        "  * Normalization = make everything have the same mean and same standard deviation.\n",
        "  * In Pytorch = https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "normalizes values over $D$ dimension, in our case, the $D$ dimension is the embedding dimension.\n",
        "    * When we normalize along the embedding dimension, it's like making all of the stairs in a staircase the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR7_eBk_RZgs"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionBlock(nn.Module):\n",
        "  \"\"\"Creates a multi-head self-attention block (\"MSA Block\").\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               embedding_dim: int=768, # Hidden size D (embedding dimension) from Table 1\n",
        "               num_heads: int=12, # Heads from Table 1\n",
        "               attn_dropout: int=0):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create the norm layer (LN)\n",
        "    self.norm_layer = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    # Create multihead attention (MSA) layer\n",
        "    self.multihead_layer = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                 dropout=attn_dropout,\n",
        "                                                 num_heads=num_heads,\n",
        "                                                 batch_first=True) # We're using (batches, num_of_patches, embedding_dim)\n",
        "                                                                   # In pytorch (batch, seq, feature)\n",
        "  def forward(self,x):\n",
        "    x = self.norm_layer(x)\n",
        "    attn_dropout, _ = self.multihead_layer(query=x,\n",
        "                                           key=x,\n",
        "                                           value=x,\n",
        "                                           need_weights=False)\n",
        "    return attn_dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xex7ixrRhnGC"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the MSA block\n",
        "multi_head_self_attention_block = MultiHeadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                              num_heads=12,\n",
        "                                                              attn_dropout=0)\n",
        "\n",
        "# Pass the patch and position image embedding sequence through MSA block\n",
        "patched_image_through_msa_block = multi_head_self_attention_block(patch_and_position_embedding)\n",
        "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
        "print(f\"Output shape of MSA block: {patched_image_through_msa_block.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKYufPANkB6Y"
      },
      "outputs": [],
      "source": [
        "patch_and_position_embedding,patched_image_through_msa_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xthZeaLU5-Mu"
      },
      "source": [
        "## 6. Equation 3: Multilayer Perceptron (MLP Block)\n",
        "\n",
        "* **MLP** = The MLP contains two layers with a GELU non-linearity (section 3.1).\n",
        "  * MLP = a quite broad term for block with a series of layer(s). Layers can be multiple or even only one hidden layer.\n",
        "  * Layers can mean: fully-connected, dense, linear, feed-forward, all are often similar names for the same thing. In PyTorch, they're often called `torch.nn.Linear()`.\n",
        "  * MLP number of hidden units = MLP Size in table 1.\n",
        "* **Dropout** = Dropout, when used, is applied after every dense (linear) layer except for the qvk-projections and directly after adding positional- to patch embeddings. Hybrid models are trained with the exact setup as their ViT counterparts.\n",
        "  * Value for Dropout available in Table 3\n",
        "\n",
        "  In pseudocode:\n",
        "  ```python\n",
        "  #MLP\n",
        "  x = linear -> non-linear -> dropout -> linear -> dropout\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV-Xa9yc8Hwd"
      },
      "outputs": [],
      "source": [
        "class MLPBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int=768,\n",
        "               mlp_size: int=3072,\n",
        "               dropout: int=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create the norm layer (LN)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    # Create the MLP\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                  out_features=mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features=mlp_size,\n",
        "                  out_features=embedding_dim),\n",
        "        nn.Dropout(p=dropout)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.mlp(self.layer_norm(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "867WwAQO-opu"
      },
      "outputs": [],
      "source": [
        "# Create an instance of MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim=embedding_dim,\n",
        "                     mlp_size=3072,\n",
        "                     dropout=0.1)\n",
        "# Pass the output of the MSABlock through the MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
        "print(f\"Output shape of MLP block: {patched_image_through_mlp_block.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeLbgiPA_WP9"
      },
      "outputs": [],
      "source": [
        "patched_image_through_msa_block,patched_image_through_mlp_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2yZqI3S-RHU"
      },
      "source": [
        "### 7. Creating the Transformer Encoder\n",
        "\n",
        "The Transformer Encoder is a combination of alternating blocks of MSA and MLP\n",
        "\n",
        "And there are residual connections between each block.\n",
        "\n",
        "* Encoder = turn a sequence into learnable representation.\n",
        "* Decoder = go from learn representation back to some sort of sequence.\n",
        "* Residual connections = add a leayer(s) input to its suibsequent output, this enables the creation of deeper networks (prevents weights from getting too small)\n",
        "\n",
        "In pseudocode:\n",
        "```python\n",
        "# Transformer Encoder\n",
        "x_input -> MSA block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + x_input] -> ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkMYlvy_EfLj"
      },
      "source": [
        "### 7.1 Create a custom Transformer Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNn42G05-RP2"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int=768,\n",
        "               num_heads: int=12,\n",
        "               mlp_size: int=3072,\n",
        "               mlp_dropout: int=0.1,\n",
        "               attn_dropout: int=0,\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create MSA block (equation 2)\n",
        "    self.msa_block = MultiHeadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                 num_heads=num_heads,\n",
        "                                                 attn_dropout=attn_dropout)\n",
        "\n",
        "    # Create MLP block (equation 3)\n",
        "    self.mlp_block = MLPBlock(embedding_dim=embedding_dim,\n",
        "                              mlp_size=mlp_size,\n",
        "                              dropout=mlp_dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.msa_block(x) # residual/skip connection for equation2\n",
        "    x = x + self.mlp_block(x) # residual/skip connection for equation3\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aNhXzj4CUhh"
      },
      "outputs": [],
      "source": [
        "# Create an instance of TransformerEncoderBlock()\n",
        "transformer_encoder_block = TransformerEncoder()\n",
        "\n",
        "# Get a summary with torchinfo\n",
        "import torchinfo\n",
        "torchinfo.summary(model=transformer_encoder_block,\n",
        "                  input_size=(1,197,768),\n",
        "                  col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
        "                  col_width=20,\n",
        "                  row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4V5Va6xEuNW"
      },
      "source": [
        "### 7.2 Create a Transformer Encoder layer with in-built PyTorch layers\n",
        "\n",
        "So far we've created a transformer encoder by hand.\n",
        "\n",
        "But because of how good the Transformer architecture, PyTorch has implemented ready to use Transformer Encoder layers: https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
        "\n",
        "We can create a Transformer Encoder with pure PyTorch layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uqO7QdhENYb"
      },
      "outputs": [],
      "source": [
        "# Create the same as above with torch.nn.TransformerEncoderLayer()\n",
        "trasnformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # embedding_dim\n",
        "                                                        nhead=12, # heads from table 1\n",
        "                                                        dim_feedforward=3072, # MLP size from table\n",
        "                                                        dropout=0.1,\n",
        "                                                        activation=\"gelu\",\n",
        "                                                       batch_first=True,\n",
        "                                                       norm_first=True)\n",
        "trasnformer_encoder_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EqwCEiNGhKG"
      },
      "outputs": [],
      "source": [
        "torchinfo.summary(model=trasnformer_encoder_layer,\n",
        "                  input_size=(1,197,768),\n",
        "                  col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
        "                  col_width=20,\n",
        "                  row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3Y1LV4M0r9_"
      },
      "source": [
        "## 8. Putting it all together to create ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LBcgVHZ0IIz"
      },
      "outputs": [],
      "source": [
        "# Create a ViT class\n",
        "class ViT(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size: int=224, # Table 3 from ViT paper\n",
        "               in_channels: int=3,\n",
        "               patch_size: int=16,\n",
        "               num_transformer_layers: int=12, # Table 1 for \"Layers\" for ViT-Base\n",
        "               embedding_dim: int=768, # Hidden size D from Table 1 for Vit-Base\n",
        "               num_heads: int=12,\n",
        "               mlp_size: int=3072,\n",
        "               mlp_dropout: int=0.1,\n",
        "               attn_dropout: int=0,\n",
        "               embedding_dropout: int=0.1,\n",
        "               num_classes: int=1000\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    # Make an assertion that the image size is compatible with the patch size\n",
        "    assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}\"\n",
        "\n",
        "    # Calculate the number of patches (height * width/patch²)\n",
        "    self.num_patches = (img_size*img_size) // patch_size**2\n",
        "\n",
        "    # Create learnable class embedding (needs to got at front of sequence)\n",
        "    self.class_embedding = nn.Parameter(torch.randn(1,1,embedding_dim),\n",
        "                                        requires_grad=True)\n",
        "\n",
        "    # Create learnable position embedding\n",
        "    self.position_embedding = nn.Parameter(data=torch.randn(1,self.num_patches+1, embedding_dim),\n",
        "                                           requires_grad=True)\n",
        "\n",
        "    # Create embedding dropout value\n",
        "    self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "    # Create patch embedding layer\n",
        "    self.patch_embedding = PatchEmbedding(input_size=in_channels,\n",
        "                                          patch_size=patch_size,\n",
        "                                          embedding_dim=embedding_dim)\n",
        "\n",
        "    # Create the Transformer Encoder Block\n",
        "    self.transformer_encoder = nn.Sequential(*[TransformerEncoder(embedding_dim=embedding_dim,\n",
        "                                                                  num_heads=num_heads,\n",
        "                                                                  mlp_size=mlp_size,\n",
        "                                                                  mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "    # Create classifier head\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                  out_features=num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Get the batch size\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    # Create class token embedding and expand it to match the batch size\n",
        "    class_token = self.class_embedding.expand(batch_size,-1,-1) # \"-1\" means to infer the dimensions\n",
        "\n",
        "    # Create the patch embedding (equation 1)\n",
        "    x = self.patch_embedding(x)\n",
        "\n",
        "    # Concat class token embedding and patch embedding (equation 1)\n",
        "    x = torch.cat((class_token,x),dim=1)\n",
        "\n",
        "    # Add position embedding to class token and patch embedding\n",
        "    x = x + self.position_embedding\n",
        "\n",
        "    # Apply dropout to patch embedding (\"directly after adding positional - to patch embeddings\")\n",
        "    x = self.embedding_dropout(x)\n",
        "\n",
        "    # Pass position and patch embedding to Transformer Encoder (equation 2 & 3)\n",
        "    x = self.transformer_encoder(x)\n",
        "\n",
        "    # Put 0th index logit through classifier (equation 4)\n",
        "    return self.classifier(x[:,0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P6RNdvx04tZ"
      },
      "outputs": [],
      "source": [
        "vit = ViT()\n",
        "vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VahtykmMneAw"
      },
      "outputs": [],
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create a random image tensor with same shape as a single image\n",
        "random_image_tensor = torch.randn(1,3,224,224)\n",
        "\n",
        "# Create an instance of ViT with the number of classes we're working with\n",
        "vit = ViT(num_classes=len(class_names))\n",
        "\n",
        "# Pass the random image tensor to our ViT instance\n",
        "vit(random_image_tensor)\n",
        "\n",
        "# Pass the random image tensor through ViT\n",
        "vit(random_image_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOiNZmIXoPVP"
      },
      "source": [
        "### 8.1 Getting a visual summary of our ViT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmV-6sXAoOgQ"
      },
      "outputs": [],
      "source": [
        "torchinfo.summary(model=ViT(num_classes=3),\n",
        "                  input_size=(1,3,224,224),\n",
        "                  col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
        "                  col_width=20,\n",
        "                  row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wvwbToIqMnP"
      },
      "source": [
        "## 9. Setting up training code for our custom ViT\n",
        "\n",
        "We've replicated the ViT architecture, now let's see how it performs on our FoodVision Mini data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-JI3cGssT-N"
      },
      "source": [
        "### 9.1 Creating an optimizer\n",
        "\n",
        "The paper states it uses the Adam optimizer (section 4, Training & fine-tuning)\n",
        "With $B1$ value of 0.9, $B2$ of 0.999 (defaults) and a weight decay of 0.1.\n",
        "\n",
        "Weight decay = Weight decay is a regularization technique by adding a small penalty, usually the L2 norm of the weights (all the weights of the model), to the loss function.\n",
        "\n",
        "Regularization technique = prevents overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbqxk_TqqLSW"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=0.001,\n",
        "                             weight_decay=0.1,\n",
        "                             betas = (0.9,0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTTLHjN0sYrd"
      },
      "source": [
        "### 9.2 Creating a loss function\n",
        "\n",
        "The ViT paper doesn't actually mention what loss function they used.\n",
        "\n",
        "So since it's a multi-class classification we'll use the `torch.nn.CrossEntropyLoss()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5KjhKwHsfk6"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4aGSjdLuwKs"
      },
      "source": [
        "### 9.3 Training our ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogI4k46Iu4Bx"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "results = engine.train(model=vit,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey7-IDF7wVsL"
      },
      "source": [
        "### 9.4 What our training setup is missing\n",
        "\n",
        "How is our training setup different to the ViT paper?\n",
        "\n",
        "We've replicated model architecture correctly.\n",
        "\n",
        "But what was different between our training procedure (to get such poor results) and the ViT paper training procedure to get such great results?\n",
        "\n",
        "The main things our training implementation is missing:\n",
        "\n",
        "Prevent underfitting:\n",
        "* Data - our setup uses fas less data (225 vs millions)\n",
        "\n",
        "Prevent overfitting:\n",
        "* Learning rate warmup - start with a low learning rate and increase to a base LR\n",
        "* Leaning rate decay - as our model gets closer to convergence, start to lower the learning rate\n",
        "* Gradient clipping - prevent gradients from getting to0 big"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fe33l3W04Qr"
      },
      "source": [
        "### 9.5 Plotting loss curver for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2yW59TK08PV"
      },
      "outputs": [],
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "plot_loss_curves(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Using a pretrained ViT from `torchvision.models`\n",
        "\n",
        "Generally, in deep learning if you can use a pretrained model from a large dataset on your own problem, it's often a good place to start.\n",
        "\n",
        "If you can fin a pretrained model and use transfer learning, give it a go, it often achieves great results with little data.\n",
        "\n",
        "### 10.1 Why use a pretrained model?\n",
        "* Sometimes data is limited\n",
        "* Limited training resources\n",
        "* Get better results faster (sometimes) ..."
      ],
      "metadata": {
        "id": "_gLNy7b2TRWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Prepare a pretrained ViT for use with FoodVision Mini (turn it into a feature extractor)"
      ],
      "metadata": {
        "id": "ib1XO24AXHnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m_GYC661EpL"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "weights = models.ViT_B_16_Weights.DEFAULT\n",
        "model = models.vit_b_16(weights=weights).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpZrTxHRJ1Ae"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "model.heads = nn.Sequential(\n",
        "    nn.Linear(in_features=768,\n",
        "              out_features=len(class_names)).to(device)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEvRz5NEKGZs"
      },
      "outputs": [],
      "source": [
        "torchinfo.summary(model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Preparing data for the pretrained ViT model\n",
        "\n",
        "When using a pretrained model, you want to make sure your data is in the same format as the data the model was trained on."
      ],
      "metadata": {
        "id": "IXp4p5xRW8a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "vit_transforms = weights.transforms()\n",
        "vit_transforms"
      ],
      "metadata": {
        "id": "gzqSnblnXbov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup dataloader\n",
        "\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir= train_dir,\n",
        "                                                                                                     test_dir= test_dir,\n",
        "                                                                                                     transform= vit_transforms,\n",
        "                                                                                                     batch_size=32)"
      ],
      "metadata": {
        "id": "pdMiF0jHXz_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNfp57M8KmAb"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(),\n",
        "                             lr=0.001)\n",
        "results = engine.train(model=model,\n",
        "            train_dataloader=train_dataloader_pretrained,\n",
        "            test_dataloader=test_dataloader_pretrained,\n",
        "            optimizer=optimizer,\n",
        "            loss_fn=loss_fn,\n",
        "            epochs=10,\n",
        "            device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4 Plot the loss curves of our pretrained ViT feature exctrator model"
      ],
      "metadata": {
        "id": "NHEKUpOBZf_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "5eN0POg8ZYTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5 Save our best perform ViT model\n",
        "\n",
        "Now we've got a model that performs quite well, how about we save it to file and check it's filesize.\n",
        "\n",
        "We want to check the filesize because if we wanted to deploy a model to say a website/mobile application, we may have limitations on the size of the model we can deploy.\n",
        "\n",
        "E.g. a smaller model may be required due to compute restrictions."
      ],
      "metadata": {
        "id": "leNF3bepaIl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"08_pretrained_vit_feature_exctractor_pizza_steak_sushi.pth\")"
      ],
      "metadata": {
        "id": "xSYxtU7DaIYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_exctractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024)\n",
        "\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size}MB\")"
      ],
      "metadata": {
        "id": "CYe181_qbp67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our pretrained ViT gets some of the best results we've seen so far on our FoodVision Mini problem. However, the model size is ~11x larger than our next best performing model.\n",
        "\n",
        "Perhaps the larger model size might cause issues when we go to deploy it (e.g. hard to deploy such a large file/might not make predictions as fast as a smaller model)."
      ],
      "metadata": {
        "id": "5Kt6mhSgffzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Predicting on a custom image"
      ],
      "metadata": {
        "id": "4oZvhQ7Fd1Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import predictions\n",
        "predictions.pred_and_plot_image(model=model,class_names=class_names,image_path=\"imagem_test.png\")"
      ],
      "metadata": {
        "id": "vIAP8-rPcdx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n"
      ],
      "metadata": {
        "id": "i9zOsKDavLx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "VK080axfwFlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = download_data(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "Bo9q8hCIw_Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "idOojzP8xYYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "IMG_SIZE = 224\n",
        "\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "manual_transforms"
      ],
      "metadata": {
        "id": "iJfZoxT0xbaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir = train_dir,\n",
        "    test_dir = test_dir,\n",
        "    transform = manual_transforms,\n",
        "    batch_size = BATCH_SIZE,\n",
        ")\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "8P-iLuZPxwBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with appropriate variables\n",
        "    def __init__(self,\n",
        "                 in_channels:int=3,\n",
        "                 patch_size:int=16,\n",
        "                 embedding_dim:int=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # 3. Create a layer to turn an image into patches\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
        "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
        "                                  end_dim=3)\n",
        "\n",
        "    # 5. Define the forward method\n",
        "    def forward(self, x):\n",
        "        # Create assertion to check that inputs are the correct shape\n",
        "        image_resolution = x.shape[-1]\n",
        "        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {self.patch_size}\"\n",
        "\n",
        "        # Perform the forward pass\n",
        "        x_patched = self.patcher(x)\n",
        "        x_flattened = self.flatten(x_patched)\n",
        "        # 6. Make sure the output shape has the right order\n",
        "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
      ],
      "metadata": {
        "id": "zSP3j-auysGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n",
        "                                                       nhead=12,\n",
        "                                                       dim_feedforward=3072,\n",
        "                                                       dropout=0.1,\n",
        "                                                       activation=\"gelu\",\n",
        "                                                       batch_first=True)\n",
        "transformer_encoder_layer"
      ],
      "metadata": {
        "id": "zhxWabRp0T-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_encoder = nn.TransformerEncoder(encoder_layer = transformer_encoder_layer,\n",
        "                                            num_layers=12)"
      ],
      "metadata": {
        "id": "Dv90SURV4dxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size: int=224,\n",
        "               num_channels: int=3,\n",
        "               patch_size: int=16,\n",
        "               embedding_dim=768,\n",
        "               dropout=0.1,\n",
        "               mlp_size=3072,\n",
        "               num_transformer_layer=12,\n",
        "               num_heads=12,\n",
        "               num_classes=1000):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0, f\"Image size must be divisible by patch size.\"\n",
        "\n",
        "    # 1. Create patch embedding\n",
        "    self.patch_embedding = PatchEmbedding(in_channels=num_channels,\n",
        "                                          patch_size=patch_size,\n",
        "                                          embedding_dim=embedding_dim)\n",
        "\n",
        "    # 2. Create class token\n",
        "    self.class_token = nn.Parameter(torch.randn(1,1,embedding_dim),\n",
        "                                    requires_grad=True)\n",
        "\n",
        "    # 3. Create positional embedding\n",
        "    num_patches = (img_size*img_size) // patch_size**2\n",
        "    self.positional_embedding = nn.Parameter(torch.rand(1,num_patches+1,embedding_dim))\n",
        "\n",
        "    # 4. Create patch + position embedding dropout\n",
        "    self.embedding_dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    # 5. Create Transformer Encoder layer (single)\n",
        "    #self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "    #                                                            nhead=num_heads,\n",
        "    #                                                            dim_feedforward=mlp_size,\n",
        "    #                                                            activation=\"gelu\",\n",
        "    #                                                            batch_first=True,\n",
        "    #                                                            norm_first=True)\n",
        "\n",
        "    # 6. Create stack Transformer Encoder layers\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "                                                                                              nhead=num_heads,\n",
        "                                                                                              dim_feedforward=mlp_size,\n",
        "                                                                                              activation=\"gelu\",\n",
        "                                                                                              batch_first=True,\n",
        "                                                                                              norm_first=True),\n",
        "                                                    num_layers=num_transformer_layer)\n",
        "\n",
        "    # 7. Create MLP head\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                  out_features=num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    x = self.patch_embedding(x)\n",
        "\n",
        "    class_token = self.class_token.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension\n",
        "\n",
        "    x = torch.cat((class_token,x),dim=1)\n",
        "\n",
        "    # Add the positional embedding to the patch embedding with class token\n",
        "    x = self.positional_embedding + x\n",
        "\n",
        "    x = self.embedding_dropout(x)\n",
        "\n",
        "    x = self.transformer_encoder(x)\n",
        "\n",
        "    return self.mlp_head(x[:,0])\n"
      ],
      "metadata": {
        "id": "YRA4LC6c2ri3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchinfo\n",
        "torchinfo.summary(ViT(num_classes=3),\n",
        "                  input_size=(1,3,224,224))"
      ],
      "metadata": {
        "id": "A5BimoJ2fVAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Turn the custom ViT architecture we created in Python script, for example, `vit.py`"
      ],
      "metadata": {
        "id": "t_B8LXYDfoZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vit.py\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with appropriate variables\n",
        "    def __init__(self,\n",
        "                 in_channels:int=3,\n",
        "                 patch_size:int=16,\n",
        "                 embedding_dim:int=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # 3. Create a layer to turn an image into patches\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
        "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
        "                                  end_dim=3)\n",
        "\n",
        "    # 5. Define the forward method\n",
        "    def forward(self, x):\n",
        "        # Create assertion to check that inputs are the correct shape\n",
        "        image_resolution = x.shape[-1]\n",
        "        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {self.patch_size}\"\n",
        "\n",
        "        # Perform the forward pass\n",
        "        x_patched = self.patcher(x)\n",
        "        x_flattened = self.flatten(x_patched)\n",
        "        # 6. Make sure the output shape has the right order\n",
        "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
        "\n",
        "class ViT(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size: int=224,\n",
        "               num_channels: int=3,\n",
        "               patch_size: int=16,\n",
        "               embedding_dim=768,\n",
        "               dropout=0.1,\n",
        "               mlp_size=3072,\n",
        "               num_transformer_layer=12,\n",
        "               num_heads=12,\n",
        "               num_classes=1000):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0, f\"Image size must be divisible by patch size.\"\n",
        "\n",
        "    # 1. Create patch embedding\n",
        "    self.patch_embedding = PatchEmbedding(in_channels=num_channels,\n",
        "                                          patch_size=patch_size,\n",
        "                                          embedding_dim=embedding_dim)\n",
        "\n",
        "    # 2. Create class token\n",
        "    self.class_token = nn.Parameter(torch.randn(1,1,embedding_dim),\n",
        "                                    requires_grad=True)\n",
        "\n",
        "    # 3. Create positional embedding\n",
        "    num_patches = (img_size*img_size) // patch_size**2\n",
        "    self.positional_embedding = nn.Parameter(torch.rand(1,num_patches+1,embedding_dim))\n",
        "\n",
        "    # 4. Create patch + position embedding dropout\n",
        "    self.embedding_dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    # 5. Create Transformer Encoder layer (single)\n",
        "    #self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "    #                                                            nhead=num_heads,\n",
        "    #                                                            dim_feedforward=mlp_size,\n",
        "    #                                                            activation=\"gelu\",\n",
        "    #                                                            batch_first=True,\n",
        "    #                                                            norm_first=True)\n",
        "\n",
        "    # 6. Create stack Transformer Encoder layers\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "                                                                                              nhead=num_heads,\n",
        "                                                                                              dim_feedforward=mlp_size,\n",
        "                                                                                              activation=\"gelu\",\n",
        "                                                                                              batch_first=True,\n",
        "                                                                                              norm_first=True),\n",
        "                                                    num_layers=num_transformer_layer)\n",
        "\n",
        "    # 7. Create MLP head\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                  out_features=num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    x = self.patch_embedding(x)\n",
        "\n",
        "    class_token = self.class_token.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension\n",
        "\n",
        "    x = torch.cat((class_token,x),dim=1)\n",
        "\n",
        "    # Add the positional embedding to the patch embedding with class token\n",
        "    x = self.positional_embedding + x\n",
        "\n",
        "    x = self.embedding_dropout(x)\n",
        "\n",
        "    x = self.transformer_encoder(x)\n",
        "\n",
        "    return self.mlp_head(x[:,0])"
      ],
      "metadata": {
        "id": "RPUVXS3ofmsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Train a pretrained ViT feature extractor model (like the one we made in 08. PyTorch Paper Replicating section 10) on 20% of the pizza, steak and sushi data like the dataset we used in 07. PyTorch Experiment Tracking section 7.3."
      ],
      "metadata": {
        "id": "BKO1PdEOhURF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT extractor model\n",
        "weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "model = torchvision.models.vit_b_16(weights=weights).to(device)\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "embedding_dim = 768\n",
        "model.heads = nn.Sequential(\n",
        "    nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "    nn.Linear(in_features=768,\n",
        "              out_features=len(class_names)).to(device)\n",
        ")"
      ],
      "metadata": {
        "id": "zJrLaO3ZhQ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 20% of the data\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi_20_percent\"\n",
        "\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "  print(f\"Did not find {image_path} directory, creating one...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(data_path / \"pizza_steak_sushi_20_percent.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\")\n",
        "  f.write(request.content)\n",
        "\n",
        "with ZipFile(data_path / \"pizza_steak_sushi_20_percent.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping pizza_steak_sushi_20_percent.zip...\")\n",
        "  zip_ref.extractall(image_path)\n",
        "  os.remove(data_path / \"pizza_steak_sushi_20_percent.zip\")\n",
        "\n",
        "# Preprocess the data\n",
        "train_dataloader_20_percent, test_dataloader_20_percent, class_names = data_setup.create_dataloaders(train_dir=image_path / \"train\",\n",
        "                                                                              test_dir=test_dir,\n",
        "                                                                              transform=weights.transforms(),\n",
        "                                                                              batch_size=1024)\n",
        "# Train model\n",
        "optimizer = torch.optim.Adam(params=model.parameters(),\n",
        "                             lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "results = engine.train(model=model,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "6K7NohQgh6Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine results\n",
        "from helper_functions import plot_loss_curves\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "GWThuM9zh6aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Try repeating the steps from excercise 3 but this time use the \"ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\" pretrained weights from `torchvision.models.vit_b_16()`."
      ],
      "metadata": {
        "id": "dmNqvgZNxOfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_swag=torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
        "model_swag = torchvision.models.vit_b_16(weights=weights_swag).to(device)\n",
        "\n",
        "for param in model_swag.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "embedding_dim = 768\n",
        "model_swag.heads = nn.Sequential(\n",
        "    nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "    nn.Linear(in_features=768,\n",
        "              out_features=len(class_names)).to(device)\n",
        ")\n",
        "\n",
        "optimizer_swag = torch.optim.Adam(params=model_swag.parameters(),\n",
        "                             lr=0.001)\n",
        "\n",
        "train_dataloader_20_percent, test_dataloader_20_percent, class_names = data_setup.create_dataloaders(train_dir=image_path / \"train\",\n",
        "                                                                              test_dir=test_dir,\n",
        "                                                                              transform=weights_swag.transforms(),\n",
        "                                                                              batch_size=32)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "results = engine.train(model=model_swag,\n",
        "                       train_dataloader=train_dataloader_20_percent,\n",
        "                       test_dataloader=test_dataloader_20_percent,\n",
        "                       optimizer=optimizer_swag,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "XM0QJvUPxVg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "WsXUR4Jyz355"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "Wyt8uyoP3yD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus: Get the \"most wrong\" examples from the test dataset"
      ],
      "metadata": {
        "id": "UjCGuhZY2V80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_labels = [path.parent.stem for path in test_data_paths]\n",
        "\n",
        "# Create a function to return a list of dictionaries with sample, label, prediction, pred prob\n",
        "def pred_and_store(test_paths, model, transform, class_names, device):\n",
        "  test_pred_list = []\n",
        "  for path in tqdm(test_paths):\n",
        "    # Create empty dict to store info for each sample\n",
        "    pred_dict = {}\n",
        "\n",
        "    # Get sample path\n",
        "    pred_dict[\"image_path\"] = path\n",
        "\n",
        "    # Get class name\n",
        "    class_name = path.parent.stem\n",
        "    pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "    # Get prediction and prediction probability\n",
        "    from PIL import Image\n",
        "    img = Image.open(path) # open image\n",
        "    transformed_image = transform(img).unsqueeze(0) # transform image and add batch dimension\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      pred_logit = model(transformed_image.to(device))\n",
        "      pred_prob = torch.softmax(pred_logit, dim=1)\n",
        "      pred_label = torch.argmax(pred_prob, dim=1)\n",
        "      pred_class = class_names[pred_label.cpu()]\n",
        "\n",
        "      # Make sure things in the dictionary are back on the CPU\n",
        "      pred_dict[\"pred_prob\"] = pred_prob.unsqueeze(0).max().cpu().item()\n",
        "      pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "    # Does the pred match the true label?\n",
        "    pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "    # print(pred_dict)\n",
        "    # Add the dictionary to the list of preds\n",
        "    test_pred_list.append(pred_dict)\n",
        "\n",
        "  return test_pred_list\n",
        "\n",
        "test_pred_dicts = pred_and_store(test_paths=test_data_paths,\n",
        "                                 model=model_swag,\n",
        "                                 transform=weights_swag.transforms(),\n",
        "                                 class_names=class_names,\n",
        "                                 device=device)\n",
        "\n",
        "test_pred_dicts[:5]\n"
      ],
      "metadata": {
        "id": "XQ-o8ejE0NZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "test_pred_df = pd.DataFrame(test_pred_dicts)\n",
        "# Sort DataFrame by correct then by pred_prob\n",
        "top_5_most_wrong = test_pred_df.sort_values(by=[\"correct\", \"pred_prob\"], ascending=[True, False]).head()\n",
        "top_5_most_wrong"
      ],
      "metadata": {
        "id": "C2v-uI602mL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "uJAm_3lv3Rj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "# Plot the top 5 most wrong images\n",
        "for row in top_5_most_wrong.iterrows():\n",
        "  row = row[1]\n",
        "  image_path = row[0]\n",
        "  true_label = row[1]\n",
        "  pred_prob = row[2]\n",
        "  pred_class = row[3]\n",
        "  # Plot the image and various details\n",
        "  img = torchvision.io.read_image(str(image_path)) # get image as tensor\n",
        "  plt.figure()\n",
        "  plt.imshow(img.permute(1, 2, 0)) # matplotlib likes images in [height, width, color_channels]\n",
        "  plt.title(f\"True: {true_label} | Pred: {pred_class} | Prob: {pred_prob:.3f}\")\n",
        "  plt.axis(False);\n"
      ],
      "metadata": {
        "id": "jbIMZq6J2m6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Our custom ViT model architecture closely mimics that of the ViT paper, however, our training recipe misses a few things. Research some of the following topics from Table 3 in the ViT paper that we miss and write a sentence about each and how it might help with training:"
      ],
      "metadata": {
        "id": "XG9BKFlT3_am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) ImageNet-21k pretraining (more data): help the model to identify different aspects from images, making it more general.\n",
        "\n",
        "2) Learning rate warmup: gradual increase in the learning rate, allowing to stabilize the model in the early stage of training and to get to larger leraning rates.\n",
        "\n",
        "3) Learning rate decay: gradual decrease in the learning rate on later stages of training.\n",
        "\n",
        "4) Gradient clipping: prevent exploding gradients by limit their size during optimization."
      ],
      "metadata": {
        "id": "dbjMBRrF0ydc"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMM7rgJ6kD1z3Zl2WT9EyrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}